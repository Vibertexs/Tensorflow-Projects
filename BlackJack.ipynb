{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlackJack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XvrCQDm8POja"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vibertexs/Tensorflow-Projects/blob/main/BlackJack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBh3dfjULo3b"
      },
      "source": [
        "# Game-Playing with Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkSmiO9Nh0ac"
      },
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib import cm\n",
        "import gym\n",
        "from IPython.display import HTML\n",
        "\n",
        "import tqdm\n",
        "\n",
        "ats = {1: \"hit\", 0:\"stay\"}\n",
        "\n",
        "def play_blackjack(epsilon, print_steps=False):\n",
        "  game = []\n",
        "  state = blackjack.reset()\n",
        "  game_over = False\n",
        "  while game_over == False:\n",
        "      if print_steps: \n",
        "        print(\"Your cards: \", blackjack.player)\n",
        "        print_state(state)\n",
        "      action = epsilon_greedy(state, epsilon)\n",
        "      if print_steps: print(f\"Agents chooses to {ats[action]}\")\n",
        "      next_state, reward, game_over, _ = blackjack.step(action)\n",
        "      game.append((state, action))\n",
        "      state = next_state\n",
        "      if game_over:\n",
        "        if print_steps:\n",
        "          print(\"Your cards: \", blackjack.player)\n",
        "          print_state(state)\n",
        "          print(\"Game over!\")\n",
        "          print(f\"Dealer has {blackjack.dealer}\")\n",
        "          print(rtm[reward])\n",
        "        game.append(reward)\n",
        "  return game\n",
        "\n",
        "\n",
        "def get_best_action(hand_val, dealer_card, usable_ace):\n",
        "    state = (hand_val, dealer_card, usable_ace)\n",
        "\n",
        "    # Compute Q(state, True)\n",
        "    if num_games[(state,True)] == 0:\n",
        "      Q_state_true = 0\n",
        "    else:\n",
        "      Q_state_true = sum_rewards[(state,True)] / num_games[(state,True)]\n",
        "    \n",
        "    # Compute Q(state, False)\n",
        "    if num_games[(state,False)] == 0:\n",
        "      Q_state_false = 0\n",
        "    else:\n",
        "      Q_state_false = sum_rewards[(state,False)] / num_games[(state,False)]\n",
        "\n",
        "    return Q_state_true >= Q_state_false\n",
        "\n",
        "def make_subplot(ax, usable_ace):\n",
        "    x_coords = np.arange(1, 11) # dealer's cards\n",
        "    y_coords = np.arange(11, 22) # gambler's hand values\n",
        "    Z = np.array([[get_best_action(y, x, usable_ace) for x in x_coords] for y in y_coords])\n",
        "    surf = ax.imshow(Z, cmap=plt.get_cmap('Set1', 2), extent=[0.5, 10.5, 21.5, 10.5])\n",
        "    plt.xticks(x_coords)\n",
        "    plt.yticks(y_coords)\n",
        "    plt.gca().invert_yaxis()\n",
        "    ax.set_xlabel('Dealer Showing')\n",
        "    ax.set_ylabel('Player Sum')\n",
        "    col1_patch = mpatches.Patch(color='darkgray', label='Hit')\n",
        "    col2_patch = mpatches.Patch(color='red', label='Stick')\n",
        "    plt.legend(handles=[col1_patch, col2_patch])\n",
        "\n",
        "def plot_strategy():\n",
        "  fig = plt.figure(figsize=(13, 13))\n",
        "  ax = fig.add_subplot(121)\n",
        "  ax.set_title('Usable Ace')\n",
        "  make_subplot(ax, True)\n",
        "  ax = fig.add_subplot(122)\n",
        "  ax.set_title('No Usable Ace')\n",
        "  make_subplot(ax, False)\n",
        "  plt.show()\n",
        "\n",
        "def best_action(state):\n",
        "\n",
        "  # Compute Q(state, True)\n",
        "  if num_games[(state,True)] == 0:\n",
        "    Q_state_true = 0\n",
        "  else:\n",
        "    Q_state_true = sum_rewards[(state,True)] / num_games[(state,True)]\n",
        "  \n",
        "  # Compute Q(state, False)\n",
        "  if num_games[(state,False)] == 0:\n",
        "    Q_state_false = 0\n",
        "  else:\n",
        "    Q_state_false = sum_rewards[(state,False)] / num_games[(state,False)]\n",
        "\n",
        "  return Q_state_true >= Q_state_false\n",
        "\n",
        "def print_state(state):\n",
        "  print()\n",
        "  print(\"Hand value:\", state[0])\n",
        "  print(\"Dealer's card:\", state[1])\n",
        "  print(\"Useable ace?:\", state[2])\n",
        "  print(\"-----\")\n",
        "\n",
        "rtm = {0:\"It's a tie!\", -1: \"You lose!\", 1:\"You win!\"}\n",
        "\n",
        "def play_n_times(blackjack, n):\n",
        "  return np.mean([play_human(blackjack) for _ in range(n)])\n",
        "\n",
        "sum_rewards = Counter()\n",
        "num_games = Counter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeIK9de2ko_"
      },
      "source": [
        "\n",
        "\n",
        "# Reinforcement Learning\n",
        "\n",
        "\n",
        "**Here are some examples of what RL can do**. \n",
        "\n",
        "\n",
        "![](https://lh3.googleusercontent.com/cTaZiZNod4_aZxSYzV-MuAzavS87y838E9Zp8BHrz65_DLi3cJCESl0sDO2ucRxLByeiJ9fQFx4kC8c-6wKivikliGgNFd7c7WIfKfQ=w1440)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s166mMlISqmI",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "69947b7a-b9f5-44cc-e1b0-ed3191af7f1c"
      },
      "source": [
        "#@title Run to see an example of a RL agent learning to play breakout!\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el79IfPCPqri"
      },
      "source": [
        "# Blackjack\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/3/33/Blackjack21.jpg)\n",
        "\n",
        "We'll use the game of blackjack . Though you can be sure that the odds are *always* against the gambler, certain play strategies are better than others. Despite Blackjack having been played since the 17th century, it was not until 1956 that a team of statisticians managed to find the provably optimal play strategy through a lengthy series of probability calculations!\n",
        "\n",
        "Black Jack Rules:\n",
        "\n",
        "\n",
        "* You and the dealer each start with two cards. You can see both of your cards and one of the dealer's cards\n",
        "* You decide to either \"hit\" or \"stay\"\n",
        "* \"hit\" means you draw another card\n",
        "* \"stay\" means you're done!\n",
        "* The goal is to make the sum of your cards higher than the dealers, but not higher than 21.\n",
        "* J, Q, K are each worth 10\n",
        "* A is worth either 1 or 11 (whichever gives you the highest sum without going over 21)\n",
        "* Dealer gets to pick cards after you're done\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2mMK6VM8T4-"
      },
      "source": [
        "blackjack = gym.make('Blackjack-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLJMfaFk8Pl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d29623-4f4d-436b-d9de-6630391ae591"
      },
      "source": [
        "play_human(blackjack)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Your cards:  [5, 4]\n",
            "Hand value: 9\n",
            "Dealer's card: 5\n",
            "Useable ace?: False\n",
            "-----\n",
            "1 for hit 0 for stay: 1\n",
            "Your cards:  [5, 4, 8]\n",
            "Hand value: 17\n",
            "Dealer's card: 5\n",
            "Useable ace?: False\n",
            "-----\n",
            "1 for hit 0 for stay: 0\n",
            "Your cards:  [5, 4, 8]\n",
            "Hand value: 17\n",
            "Dealer's card: 5\n",
            "Useable ace?: False\n",
            "-----\n",
            "Game over!\n",
            "Dealer has [5, 1, 5]\n",
            "You lose!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvrCQDm8POja"
      },
      "source": [
        "# Defining the world\n",
        "\n",
        "\n",
        "These interact together to define our world as follows!\n",
        "\n",
        "![](https://dzone.com/storage/temp/6976061-screen-shot-2017-10-20-at-22200-pm.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lu3XkyERndf"
      },
      "source": [
        "In blackjack, we'll define a **state** using the following format:\n",
        "\n",
        "$s = $`(sum_of_cards_in_hand, dealers_card, useable_ace)`\n",
        "\n",
        "where:\n",
        "\n",
        "*   `sum_of_cards_in_hand` is sum of cards in hand (integer)\n",
        "*   `dealers_card` is the dealer's visible card (integer) \n",
        "*   `useable_ace` is whether or not we have a useable ace (boolean)\n",
        "\n",
        "The **actions** are to hit or to stay.\n",
        "\n",
        "Because we are interested in training our agent to win as much as possible, we'll define a simple **reward** function which is $+1$ upon winning, $-1$ upon losing, and $0$ at all intermediate (i.e. non-game-ending) states, as well as upon tieing. We might make several moves before we get get a non-zero reward!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0avSFZBB4_"
      },
      "source": [
        "# Policy\n",
        "\n",
        "We're trying to learn a **policy**: rules for how we'll select **actions** based on the **state** of the world. The objective is to learn a policy that gets us the largest **reward** possible!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSk5nmDDufk5"
      },
      "source": [
        "# Value\n",
        "\n",
        "Let's define the **value** $Q(s, a)$ of an action $a$, at state $s$ as the expected future reward of taking an action at a certain state. Think $Q$ for quality: what payoff should we expect with this move?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhm_SMJBhDn-"
      },
      "source": [
        "Note that in real life we don't get a nice dictionary with our value function, so we'll have to learn it! This is the core idea behind our algorithm for today, which is called **Q-learning**. By experimenting with different actions, we'll learn what tends to happen when we make an action from a particular state.\n",
        "\n",
        "\n",
        "# Q-Learning - Estimating Value\n",
        "\n",
        "In order to learn $Q$, we'll repeat the following two steps:\n",
        "1. Run an episode: have the agent interact with the world in some way (more on how later!)\n",
        "2. Update the value, $Q(s, a)$, for each $(s, a)$ pair that occurred in the episode based on the rewards that we got!\n",
        "\n",
        "For blackjack this is: \n",
        "1.   Play a game of Blackjack.\n",
        "2.   Update the $Q(s,a)$ value, for each $(s,a)$ pair that occurred in the game, based on whether we won or lost at the end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovvW4r8xzg-J"
      },
      "source": [
        "## Updating the Value Estimates\n",
        "\n",
        "We can represent our game of blackjack like this:\n",
        "\n",
        "$s_1, a_1, s_2, a_2, \\ldots, s_n, a_n, r$\n",
        "\n",
        "Where $r$ is the reward received at the end of the game: we're in a state, we take an action, repeat. Eventually, the game ends and we get a reward.\n",
        "\n",
        "One complication here is that Blackjack games last for a variable number of moves, so we don't know what $n$ is upfront!\n",
        "\n",
        "Suppose that, in code, this gameplay data is given you in the following form:\n",
        "\n",
        "`game = [(s_1,a_1), (s_2,a_2), ... , (s_n,a_n), r]`\n",
        "\n",
        "That is, `game` is a `List` of $n$ `Tuples` (each of which contains a state and an action) followed by a single reward at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMd4vJ9ynoXt"
      },
      "source": [
        "We'll keep a tally of the rewards we get for each action in each state, so that later we can find the average value.\n",
        "\n",
        "**Exercise:** Write a function that takes a `game` as argument and updates `sum_rewards` and `num_games` appropriately.\n",
        "* `sum_rewards[(s, a)]` is the total reward at the state `(s, a)`.\n",
        "* `num_games[(s, a)]` is the number of times we've seen the state `(s, a)`.\n",
        "\n",
        "At the start, the two dictionaries are both initialized so that the value for each `(s, a)` pair is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JfBDdE8n2EY"
      },
      "source": [
        "def update_counters(game):\n",
        "  reward = game[-1]\n",
        "  for state_action_tuple in game[:-1]: #tuple is (s,a)\n",
        "    sum_rewards[state_action_tuple] += reward\n",
        "    num_games[state_action_tuple] +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzLKitdlkuW"
      },
      "source": [
        "# Choosing Actions\n",
        "\n",
        "Now we've got a way to learn the value of each action from each state!\n",
        "\n",
        "But how do we use that to actually play the game while we're still learning? (What policy does the agent use while it is training?)\n",
        "\n",
        "Following strategies:\n",
        "* **Explore:** At any state, just randomly select an action!\n",
        "* **Exploit:** At any state, pick the action we currently think is the best based on our value estimate! (this is the q_policy_implementation that we wrote!)\n",
        "\n",
        "![](https://asopa.typepad.com/.a/6a00d83454f2ec69e2022ad3b982a8200b-800wi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKGXAvTazfnR"
      },
      "source": [
        "### Epsilon Greedy ($\\varepsilon$-greedy) approach!\n",
        "\n",
        "There is a trade-off between the two strategies above, so we compromise and mix the two strategies together! We'll use something called the $\\varepsilon$-greedy algorithm! This algorithm explores (selects a random action) with probability $\\varepsilon$, and exploits (selects the best action) with probability $(1-\\varepsilon)$. \n",
        "\n",
        "**Exercise:** Read the code below to see how this policy is implemented!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcIimTixzyGK"
      },
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "\n",
        "  # Return best_action with probability 1-epsilon\n",
        "  if np.random.rand() > epsilon:\n",
        "    return best_action(state)\n",
        "  \n",
        "  # Otherwise, return random action\n",
        "  else:\n",
        "    return np.random.choice([0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LioUt8hnlfy3"
      },
      "source": [
        "# Training our Agent\n",
        "\n",
        "Time to put it all together and actually train our agent to learn a good blackjack strategy!\n",
        "\n",
        "We'll use the `play_blackjack(epsilon)` function which plays the game of blackjack using the epsilon_greedy policy, and then update our counters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNB8AOHY7TtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3428b784-39b7-4f23-b449-999731758cbb"
      },
      "source": [
        "epsilon = 0.15\n",
        "sum_rewards = Counter()\n",
        "num_games = Counter()\n",
        "\n",
        "for iteration in tqdm.tqdm(range(1000000)):\n",
        "  game = play_blackjack(epsilon)\n",
        "  update_counters(game)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [01:01<00:00, 16355.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inDFbi1HFCtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6338732-0df2-4097-f91d-a10149cf4ae9"
      },
      "source": [
        "play_blackjack(epsilon=0, print_steps=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your cards:  [10, 4]\n",
            "\n",
            "Hand value: 14\n",
            "Dealer's card: 10\n",
            "Useable ace?: False\n",
            "-----\n",
            "Agents chooses to hit\n",
            "Your cards:  [10, 4, 10]\n",
            "\n",
            "Hand value: 24\n",
            "Dealer's card: 10\n",
            "Useable ace?: False\n",
            "-----\n",
            "Game over!\n",
            "Dealer has [10, 2]\n",
            "You lose!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((14, 10, False), True), -1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu8_JP628kp4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "ce307486-1231-4e25-d48f-8ad419f1a7c2"
      },
      "source": [
        "plot_strategy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_XiNWcl6n3G"
      },
      "source": [
        "Compare the strategy that your agent learned to the optimal Blackjack strategy, shown below on the left. It should look pretty similar!\n",
        "\n",
        "(Notice that the grid layouts differ a bit between our visualization and the one below; we plot the best actions aligned with the tick marks, whereas the following figure plots the best actions in between tick marks).\n",
        "\n",
        "\n",
        "![alt text](https://ankonzoid.github.io/LearningX/classical_RL/blackjack/images/coverart.png)\n"
      ]
    }
  ]
}